{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Spectral Methods Project - Random SVD.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXmsksnaIPUK",
    "colab_type": "text"
   },
   "source": [
    "#Spectral Methods in Data Processing (Fall 2019) - Final Project\n",
    "## Random SVD and Random ID Results Reconstruction- Elad Eatah\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/RedCrow9564/SpectralMethodsProject-RandomSVD/blob/master/Spectral_Methods_Project_Random_SVD.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>\n",
    "[![MIT License](https://img.shields.io/apm/l/atomic-design-ui.svg?)](https://github.com/tterb/atomic-design-ui/blob/master/LICENSEs)\n",
    "\n",
    "Complete description of this project is avilable in [this repo](https://github.com/RedCrow9564/SpectralMethodsProject-RandomSVD.git).\n",
    "\n",
    "## Getting Started\n",
    "First run the nodes under \"Infrastructure\" one by one.\n",
    "Second, run the first two nodes under \"Main Components\".\n",
    "\n",
    "### Running Unit-Tests\n",
    "You can then run the nodes under \"UnitTests\" one by one. The results of the last node are the results of these Unit-Tests.\n",
    "\n",
    "### Performing an experiment\n",
    "This is done by running the \"main\" node. The number of the experiment to perform can be set in the form. It can be any integer between 1 and 5. Its results are then saved to a local directory in the remote machine. Make sure this directory exists on the remote machine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtiAVIb7-AnH",
    "colab_type": "text"
   },
   "source": [
    "#Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eVgSZWIORI_k",
    "colab_type": "code",
    "cellView": "form",
    "outputId": "5a1b865d-4c80-4a92-cf73-5b3d9dd1e92f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    }
   },
   "source": [
    "#@title Dependencies installations\n",
    "!pip install pandas nptyping sacred\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import warnings\n",
    "import psutil\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyximport\n",
    "import cython\n",
    "from sacred import Experiment\n",
    "from time import perf_counter\n",
    "import cpuinfo\n",
    "#warnings.filterwarnings(\"error\")  # Uncomment to see warnings as errors.\n",
    "%load_ext Cython\n",
    "\n",
    "# Defining the \"sacred\" experiment object.\n",
    "ex = Experiment(name=\"Initializing project\", interactive=True)\n",
    "\n",
    "cpu_info = cpuinfo.get_cpu_info()\n",
    "cpu_count: int = cpu_info['count']\n",
    "clear_output()\n",
    "print(cpu_info['brand'])\n",
    "print(f'Total CPUs: {cpu_count}')\n",
    "print(\"Installation is done!\")\n"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "Total CPUs: 2\n",
      "Installation is done!\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3BGo1w8f-ZIu",
    "colab_type": "code",
    "cellView": "form",
    "outputId": "0d0927d2-0fe1-4591-b467-109a727260f9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "#@title Common Utils\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "utils.py - The common utilities functions and objects\n",
    "=====================================================\n",
    "\n",
    "This module contains all frequently-used methods and objects which can be shared among the entire project.\n",
    "For example, data types name used for type-hinting, a basic enum class :class:`BaseEnum`, methods for measuring\n",
    "run-time of a given function.\n",
    "\"\"\"\n",
    "from typing import List, Dict, Callable, Union, Iterator\n",
    "from nptyping import Array\n",
    "import inspect\n",
    "\n",
    "pyximport.install(setup_args={\"include_dirs\": np.get_include()}, \n",
    "                  reload_support=True)\n",
    "\n",
    "# Naming data types for type hinting.\n",
    "Number = Union[int, float]\n",
    "Scalar = Union[Number, Array[float, 1, 1], Array[int, 1, 1]]\n",
    "RowVector = Union[List[Scalar], Array[float, 1, ...], Array[int, 1, ...], Scalar]\n",
    "ColumnVector = Union[List[Scalar], Array[float, ..., 1], Array[int, ..., 1], Scalar]\n",
    "Vector = Union[RowVector, ColumnVector]\n",
    "Matrix = Union[List[Vector], Array[float], Array[int], Vector, Scalar]\n",
    "\n",
    "\n",
    "class _MetaEnum(type):\n",
    "    \"\"\"\n",
    "    A private meta-class which given any :class:`BaseEnum` object to be an iterable.\n",
    "    This can be used for iterating all possible values of this enum. Should not be used explicitly.\n",
    "    \"\"\"\n",
    "    def __iter__(self) -> Iterator:\n",
    "        \"\"\"\n",
    "        This method gives any BaseEnum the ability of iterating over all the enum's values.\n",
    "\n",
    "        Returns:\n",
    "            An iterator for the collection of all the enum's values.\n",
    "\n",
    "        \"\"\"\n",
    "        # noinspection PyUnresolvedReferences\n",
    "        return self.enum_iter()\n",
    "\n",
    "    def __contains__(self, item) -> bool:\n",
    "        \"\"\"\n",
    "        This method give any BaseEnum the ability to test if a given item is a possible value for this enum class.\n",
    "\n",
    "        Returns:\n",
    "            A flag which indicates if 'item' is a possible value for this enum class.\n",
    "\n",
    "        \"\"\"\n",
    "        # noinspection PyUnresolvedReferences\n",
    "        return self.enum_contains(item)\n",
    "\n",
    "\n",
    "class BaseEnum(metaclass=_MetaEnum):\n",
    "    \"\"\"\n",
    "    A basic interface for all enum classes. Should be sub-classed in eny enum, i.e ``class ExperimentType(BaseEnum)``\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def enum_iter(cls) -> Iterator:\n",
    "        \"\"\"\n",
    "        This method gives any BaseEnum the ability of iterating over all the enum's values.\n",
    "\n",
    "        Returns:\n",
    "            An iterator for the collection of all the enum's values.\n",
    "\n",
    "        \"\"\"\n",
    "        return iter(cls.get_all_values())\n",
    "\n",
    "    @classmethod\n",
    "    def enum_contains(cls, item) -> bool:\n",
    "        \"\"\"\n",
    "        This method give any BaseEnum the ability to test if a given item is a possible value for this enum class.\n",
    "\n",
    "        Returns:\n",
    "            A flag which indicates if 'item' is a possible value for this enum class.\n",
    "\n",
    "        \"\"\"\n",
    "        return item in cls.get_all_values()\n",
    "\n",
    "    @classmethod\n",
    "    def get_all_values(cls) -> List:\n",
    "        \"\"\"\n",
    "        A method which fetches all possible values of an enum. Used for iterating over an enum.\n",
    "\n",
    "        Returns:\n",
    "            A list of all possible enum's values.\n",
    "\n",
    "        \"\"\"\n",
    "        all_attributes: List = inspect.getmembers(cls, lambda a: not inspect.ismethod(a))\n",
    "        all_attributes = [value for name, value in all_attributes if not (name.startswith('__') or name.endswith('__'))]\n",
    "        return all_attributes\n",
    "\n",
    "\n",
    "def create_factory(possibilities_dict: Dict[str, Callable], are_methods: bool = False) -> Callable:\n",
    "    \"\"\"\n",
    "    A generic method for creating factories for the entire project.\n",
    "\n",
    "    Args:\n",
    "        possibilities_dict(Dict[str, Callable]): The dictionary which maps object types (as strings!) and returns the\n",
    "            relevant class constructors.\n",
    "        are_methods(bool): A flag, true if the factory output are methods, rather than objects. Defaults to False\n",
    "\n",
    "    Returns:\n",
    "         The factory function for the given classes/methods mapping.\n",
    "\n",
    "    \"\"\"\n",
    "    def factory_func(requested_object_type: str):  # Inner function!\n",
    "        if requested_object_type not in possibilities_dict:\n",
    "            raise ValueError(\"Object type {0} is NOT supported\".format(requested_object_type))\n",
    "        else:\n",
    "            if are_methods:\n",
    "                return possibilities_dict[requested_object_type]\n",
    "            else:\n",
    "                return possibilities_dict[requested_object_type]()\n",
    "\n",
    "    return factory_func\n",
    "\n",
    "\n",
    "def measure_time(method: Callable) -> Callable:\n",
    "    \"\"\"\n",
    "    A method which receives a method and returns the same method, while including run-time measure\n",
    "    output for the given method, in seconds.\n",
    "\n",
    "    Args:\n",
    "        method(Callable): A method whose run-time we are interested in measuring.\n",
    "\n",
    "    Returns:\n",
    "        A function which does exactly the same, with an additional run-time output value in seconds.\n",
    "\n",
    "    \"\"\"\n",
    "    def timed(*args, **kw):\n",
    "        ts = perf_counter()\n",
    "        result = method(*args, **kw)\n",
    "        te = perf_counter()\n",
    "        duration_in_seconds: float = te - ts\n",
    "        return result + (duration_in_seconds,)\n",
    "    timed.__name__ = method.__name__ + \" with time measure\"\n",
    "    return timed\n",
    "\n",
    "\n",
    "def is_empty(collection: List) -> bool:\n",
    "    return len(collection) == 0\n",
    "\n",
    "\n",
    "class DataLog:\n",
    "    \"\"\"\n",
    "    A class for log-management objects. See the following example for creating it: ``DataLog([\"Column 1\", \"Column 2\"])``\n",
    "    \"\"\"\n",
    "    def __init__(self, log_fields: List):\n",
    "        \"\"\"\n",
    "        This methods initializes an empty log.\n",
    "\n",
    "        Args:\n",
    "            log_fields(List) - A list of column names for this log.\n",
    "\n",
    "        \"\"\"\n",
    "        self._data: Dict = dict()\n",
    "        self._log_fields: List = log_fields\n",
    "        for log_field in log_fields:\n",
    "            self._data[log_field] = []\n",
    "\n",
    "    def append(self, data_type: str, value: Scalar) -> None:\n",
    "        \"\"\"\n",
    "        This methods appends given data to the given column inside the log.\n",
    "        Example of usage:``log.append(DataFields.DataSize, 20)``\n",
    "\n",
    "        Args:\n",
    "            data_type(LogFields): The column name in which the input data in inserted to.\n",
    "            value(Scalar): The value to insert to the log.\n",
    "\n",
    "        \"\"\"\n",
    "        self._data[data_type].append(value)\n",
    "\n",
    "    def append_dict(self, data_dict: Dict) -> None:\n",
    "        \"\"\"\n",
    "        This methods takes the data from the input dictionary and inserts it to this log.\n",
    "\n",
    "        Args:\n",
    "            data_dict(Dict): The dictionary from which new data is taken and inserted to the log.\n",
    "\n",
    "        \"\"\"\n",
    "        for log_field, data_value in data_dict.items():\n",
    "            self.append(log_field, data_value)\n",
    "\n",
    "    def save_log(self, log_file_name: str, results_folder_path: str) -> None:\n",
    "        \"\"\"\n",
    "        This method saves the log to a file, with the input name, in the input folder path.\n",
    "\n",
    "        Args:\n",
    "            log_file_name(str): The name for this log file.\n",
    "            results_folder_path(str): The path in which this log will be saved.\n",
    "\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(self._data, columns=self._log_fields)\n",
    "        df.to_csv(os.path.join(results_folder_path, log_file_name + \".csv\"), sep=\",\", float_format=\"%.2E\", index=False)\n",
    "        ex.info[\"Experiment Log\"] = self._data\n",
    "\n",
    "print(\"Utils loaded successfully!\")\n"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Utils loaded successfully!\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XINvJcJU_IZQ",
    "colab_type": "code",
    "cellView": "form",
    "outputId": "7f3569b0-6d36-4b19-8728-990e6b945a56",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "#@title Enums\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "enums.py - All enums section\n",
    "============================\n",
    "\n",
    "This module contains all possible enums of this project. Most of them are used by the configuration section in\n",
    ":mod:`main`. An example for using enum: ``ExperimentType.ExampleNo1``\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from typing import Iterator, List\n",
    "\n",
    "\n",
    "class LogFields(BaseEnum):\n",
    "    \"\"\"\n",
    "    The enum class of fields within experiments logs. Possible values:\n",
    "\n",
    "    * ``LogFields.DataSize``\n",
    "\n",
    "    * ``LogFields.ApproximationRank``\n",
    "\n",
    "    * ``LogFields.Increment``\n",
    "\n",
    "    * ``LogFields.NextSingularValue``\n",
    "\n",
    "    * ``LogFields.RandomSVDDuration``\n",
    "\n",
    "    * ``LogFields.RandomIDDuration``\n",
    "\n",
    "    * ``LogFields.RandomSVDAccuracy``\n",
    "\n",
    "    * ``LogFields.RandomIDAccuracy``\n",
    "    \"\"\"\n",
    "    DataSize: str = \"Data size\"\n",
    "    ApproximationRank: str = \"k\"\n",
    "    Increment: str = \"increment\"\n",
    "    NextSingularValue: str = \"K+1 singular value\"\n",
    "    RandomSVDDuration: str = \"Random SVD Duration in seconds\"\n",
    "    RandomIDDuration: str = \"Random ID Duration in seconds\"\n",
    "    RandomSVDAccuracy: str = \"Random SVD Accuracy\"\n",
    "    RandomIDAccuracy: str = \"Random ID Accuracy\"\n",
    "\n",
    "\n",
    "class ExperimentType(BaseEnum):\n",
    "    \"\"\"\n",
    "    The enum class of experiment types. Possible values:\n",
    "\n",
    "    * ``ExperimentType.ExampleNo1``\n",
    "\n",
    "    * ``ExperimentType.ExampleNo2``\n",
    "\n",
    "    * ``ExperimentType.ExampleNo3``\n",
    "\n",
    "    * ``ExperimentType.ExampleNo4``\n",
    "\n",
    "    * ``ExperimentType.ExampleNo5``\n",
    "\n",
    "    \"\"\"\n",
    "    ExampleNo1: str = \"Example No. 1\"\n",
    "    ExampleNo2: str = \"Example No. 2\"\n",
    "    ExampleNo3: str = \"Example No. 3\"\n",
    "    ExampleNo4: str = \"Example No. 4\"\n",
    "    ExampleNo5: str = \"Example No. 5\"\n",
    "\n",
    "print(\"Enums loaded successfully!\")\n"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Enums loaded successfully!\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21Q34m4y_pvD",
    "colab_type": "text"
   },
   "source": [
    "#Main Components"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TQVRli1R_v-y",
    "colab_type": "code",
    "cellView": "form",
    "outputId": "4ed962ee-04aa-4fe8-d65c-50f302be0b06",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "#@title Data Loading for Experiments\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\" The data management module\n",
    "\n",
    "This module handles the fetching of the data from the local resources path, given in the configuration and arranging it\n",
    "for our purposes of estimations. See the example for fetching the data for Example no. 1.\n",
    "\n",
    "Example:\n",
    "    get_data(ExperimentType.ExampleNo1) - Creating the data for Example no. 1 of the paper.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "\n",
    "def _random_orthonormal_cols(data_size: int, columns: int) -> Matrix:\n",
    "    return ortho_group.rvs(data_size, size=1)[:, :columns]\n",
    "\n",
    "\n",
    "def _get_first_3_examples_data(data_size: int, singular_values: RowVector) -> Matrix:\n",
    "    \"\"\"\n",
    "    A method which creates a random matrix of size data_size x data_size with given singular values.\n",
    "\n",
    "    Args:\n",
    "        data_size(int): The input data size n.\n",
    "        singular_values(RowVector): The singular values to be set for the matrix to create.\n",
    "\n",
    "    Returns:\n",
    "        A random size data_size x data_size Matrix awith the given singular values.\n",
    "\n",
    "    \"\"\"\n",
    "    rank: int = len(singular_values)\n",
    "    U: Matrix = _random_orthonormal_cols(data_size, rank)\n",
    "    VT: Matrix = _random_orthonormal_cols(data_size, rank).T\n",
    "    return U.dot(np.diag(singular_values).dot(VT))\n",
    "\n",
    "\n",
    "def _get_example_4_data(data_size: int, singular_values: RowVector) -> Matrix:\n",
    "    \"\"\"\n",
    "    A method which creates a data_size x data_size matrix whose singular values are the input values.\n",
    "\n",
    "    Args:\n",
    "        data_size(int): The input data size n.\n",
    "        singular_values(RowVector): The singular values to be set for the matrix to create.\n",
    "\n",
    "    Returns:\n",
    "        A data_size x data_size Matrix with the given singular values.\n",
    "\n",
    "    \"\"\"\n",
    "    U: Matrix = np.stack([np.ones(data_size),\n",
    "                          np.tile([1, -1], data_size // 2),\n",
    "                          np.tile([1, 1, -1, -1], data_size // 4),\n",
    "                          np.tile([1, 1, 1, 1, -1, -1, -1, -1], data_size // 8)]).T / np.sqrt(data_size)\n",
    "    VT: Matrix = np.stack([np.concatenate([np.ones(data_size - 1), [0]]) / np.sqrt(data_size - 1),\n",
    "                           np.concatenate([np.zeros(data_size - 1), [1]]),\n",
    "                           np.concatenate([np.tile([1, -1], (data_size - 2) // 2) / np.sqrt(data_size - 2), [0, 0]]),\n",
    "                           np.concatenate([[1, 0, -1], np.zeros(data_size - 3)]) / np.sqrt(2)])\n",
    "    return U.dot(np.diag(singular_values).dot(VT))\n",
    "\n",
    "\n",
    "def _get_example_5_data(data_size: int, singular_values: RowVector) -> Matrix:\n",
    "    \"\"\"\n",
    "    A method which creates a data_size x data_size matrix with singular values 1 and the other input singular values.\n",
    "\n",
    "    Args:\n",
    "        data_size(int): The input data size n.\n",
    "        singular_value(RowVector): A 1x2 vector of singular values for the created matrix.\n",
    "\n",
    "    Returns:\n",
    "        A random size data_size x data_size Matrix with singular values 1 and the other input singular value.\n",
    "\n",
    "    \"\"\"\n",
    "    data: Matrix = singular_values[1] * np.eye(data_size)\n",
    "    data[0, :] += singular_values[0] * np.ones(data_size) / np.sqrt(data_size)\n",
    "    return data\n",
    "\n",
    "\n",
    "# A private dictionary used to create the method \"get_data\"\n",
    "_data_type_to_function: Dict[str, Callable] = {\n",
    "    ExperimentType.ExampleNo1: _get_first_3_examples_data,\n",
    "    ExperimentType.ExampleNo2: _get_first_3_examples_data,\n",
    "    ExperimentType.ExampleNo3: _get_first_3_examples_data,\n",
    "    ExperimentType.ExampleNo4: _get_example_4_data,\n",
    "    ExperimentType.ExampleNo5: _get_example_5_data\n",
    "}\n",
    "\n",
    "# The public method which fetches the data loading methods.\n",
    "get_data: Callable = create_factory(_data_type_to_function, are_methods=True)\n",
    "\n",
    "print(\"Data loading Methods loaded successfully!\")\n"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Data loading Methods loaded successfully!\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kByeZ0FoAV9r",
    "colab_type": "code",
    "cellView": "form",
    "outputId": "1804bd44-c096-400f-9aa2-9554090cc314",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "#@title Randomized Decomposition Algorithms\n",
    "%%cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "cimport cython\n",
    "from numpy.linalg import svd\n",
    "from scipy.linalg.interpolative import interp_decomp, reconstruct_interp_matrix\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.nonecheck(False)\n",
    "@cython.cdivision(True)\n",
    "@cython.initializedcheck(False)\n",
    "def random_svd(np.ndarray[double, ndim=2] A, const int k, const int increment):\n",
    "    cdef Py_ssize_t m = A.shape[0]\n",
    "    cdef np.ndarray[double, ndim=1] sigma\n",
    "    cdef np.ndarray[double, ndim=2] Q, VT, U\n",
    "\n",
    "    _, _, Q = svd(np.random.randn(k + increment, m).dot(A), full_matrices=False,\n",
    "                  compute_uv=True)\n",
    "    Q = Q[:k, :].T\n",
    "    U, sigma, VT = svd(A.dot(Q), full_matrices=False, compute_uv=True)\n",
    "    VT = Q.dot(VT.T).T\n",
    "    return U, sigma, VT\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.nonecheck(False)\n",
    "@cython.cdivision(True)\n",
    "@cython.initializedcheck(False)\n",
    "def random_id(np.ndarray[double, ndim=2] A, const int k, const int increment):\n",
    "    cdef Py_ssize_t m = A.shape[0]\n",
    "    cdef np.ndarray[int, ndim=1] idx\n",
    "    cdef np.ndarray[double, ndim=2] B, P, proj\n",
    "\n",
    "    idx, proj = interp_decomp(np.random.randn(k + increment, m).dot(A), k, \n",
    "                              rand=False)\n",
    "    P = reconstruct_interp_matrix(idx, proj)\n",
    "    B = A[:, idx[:k]]\n",
    "    return B, P\n",
    "\n",
    "print(\"Randommized Algorithms loaded successfully!\")\n"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Randommized Algorithms loaded successfully!\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fHJS-ZIoADPz",
    "colab_type": "code",
    "cellView": "form",
    "outputId": "fb843514-6b52-4336-fe42-49d52f5f1e8f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    }
   },
   "source": [
    "#@title Main\n",
    "experiment_number_from_one_to_five = 1 #@param {type:\"integer\"}\n",
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "main.py - The main module of the project\n",
    "========================================\n",
    "\n",
    "This module contains the config for the experiment in the \"config\" function.\n",
    "Running this module invokes the :func:`main` function, which then performs the experiment and saves its results\n",
    "to the configured results folder. Example for running an experiment: ``python -m main.py``\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def choose_singular_values(experiment_type: ExperimentType) -> RowVector:\n",
    "    \"\"\"\n",
    "\n",
    "    This function sets the needed singular values, according to the given experiment_type\n",
    "\n",
    "    Args:\n",
    "        experiment_type(ExperimentType): The performed experiment. For example, ``ExperimentType.ExampleNo1``.\n",
    "\n",
    "    Returns:\n",
    "        A RowVector of the required singular values.\n",
    "\n",
    "    \"\"\"\n",
    "    if experiment_type == ExperimentType.ExampleNo1:\n",
    "        return np.concatenate([np.flip(np.geomspace(0.2e-15, 1, num=10)), 0.2e-15 * np.ones(10)])\n",
    "    elif experiment_type == ExperimentType.ExampleNo2:\n",
    "        return np.concatenate([np.flip(np.geomspace(1e-8, 1, num=10)), 1e-8 * np.ones(10)])\n",
    "    elif experiment_type == ExperimentType.ExampleNo3:\n",
    "        return np.concatenate([np.flip(np.geomspace(1e-9, 1, num=30)), 1e-9 * np.ones(30)])\n",
    "    elif experiment_type == ExperimentType.ExampleNo4:\n",
    "        return [1, 1, 1e-8, 1e-8]\n",
    "    elif experiment_type == ExperimentType.ExampleNo5:\n",
    "        return [1, 1e-17]\n",
    "    return [-1]\n",
    "\n",
    "\n",
    "def choose_increments(experiment_type: ExperimentType) -> List:\n",
    "    \"\"\"\n",
    "\n",
    "    This function sets the needed increments, according to the given experiment_type\n",
    "\n",
    "    Args:\n",
    "        experiment_type(ExperimentType): The performed experiment. For example, ``ExperimentType.ExampleNo1``.\n",
    "\n",
    "    Returns:\n",
    "        A list of the required increments.\n",
    "\n",
    "    \"\"\"\n",
    "    if experiment_type in [ExperimentType.ExampleNo1, ExperimentType.ExampleNo2,\n",
    "                           ExperimentType.ExampleNo4, ExperimentType.ExampleNo5]:\n",
    "        return [0]\n",
    "    elif experiment_type == ExperimentType.ExampleNo3:\n",
    "        return [0] + np.round(np.geomspace(2, 16, 4)).astype(int).tolist()\n",
    "    return [-1]\n",
    "\n",
    "\n",
    "def choose_approximation_ranks(experiment_type: ExperimentType) -> List:\n",
    "    \"\"\"\n",
    "\n",
    "    This function sets the needed approximation ranks, according to the given experiment_type\n",
    "\n",
    "    Args:\n",
    "        experiment_type(ExperimentType): The performed experiment. For example, ``ExperimentType.ExampleNo1``.\n",
    "\n",
    "    Returns:\n",
    "        A list of the required approximation ranks.\n",
    "\n",
    "    \"\"\"\n",
    "    if experiment_type in [ExperimentType.ExampleNo1, ExperimentType.ExampleNo2, ExperimentType.ExampleNo5]:\n",
    "        return [10]\n",
    "    elif experiment_type == ExperimentType.ExampleNo3:\n",
    "        return [30]\n",
    "    elif experiment_type == ExperimentType.ExampleNo4:\n",
    "        return [2]\n",
    "    return [-1]\n",
    "\n",
    "\n",
    "def choose_data_sizes(experiment_type: ExperimentType) -> List:\n",
    "    \"\"\"\n",
    "\n",
    "    This function sets the needed data sizes, according to the given experiment_type\n",
    "\n",
    "    Args:\n",
    "        experiment_type(ExperimentType): The performed experiment. For example, ``ExperimentType.ExampleNo1``.\n",
    "\n",
    "    Returns:\n",
    "        A list of the required data sizes.\n",
    "\n",
    "    \"\"\"\n",
    "    if experiment_type in [ExperimentType.ExampleNo1, ExperimentType.ExampleNo2, ExperimentType.ExampleNo5]:\n",
    "        return np.geomspace(1e+2, 1e+4, 3, dtype=int).tolist()\n",
    "    elif experiment_type == ExperimentType.ExampleNo3:\n",
    "        return [1e+5]\n",
    "    elif experiment_type == ExperimentType.ExampleNo4:\n",
    "        return (4 * np.geomspace(1e+2, 1e+4, 3, dtype=int)).tolist()\n",
    "    return [-1]\n",
    "\n",
    "\n",
    "@ex.config\n",
    "def config():\n",
    "    \"\"\" Config section\n",
    "\n",
    "    This function contains all possible configuration for all experiments. Full details on each configuration values\n",
    "    can be found in :mod:`enums.py`.\n",
    "    \"\"\"\n",
    "\n",
    "    experiment_type: str = f'Example No. {experiment_number_from_one_to_five}'\n",
    "    singular_values: RowVector = choose_singular_values(experiment_type)\n",
    "    used_data_factory: Callable = get_data(experiment_type)\n",
    "    data_sizes: List = choose_data_sizes(experiment_type)\n",
    "    approximation_ranks: List = choose_approximation_ranks(experiment_type)\n",
    "    increments: List = choose_increments(experiment_type)\n",
    "    results_path: str = r'Results/'\n",
    "\n",
    "\n",
    "@ex.main\n",
    "def main(data_sizes: List, approximation_ranks: List, increments: List, singular_values: RowVector,\n",
    "         used_data_factory: Callable, results_path: str, experiment_type: str) -> DataLog:\n",
    "    \"\"\" The main function of this project\n",
    "\n",
    "    This functions performs the desired experiment according to the given configuration.\n",
    "    The function runs the random_svd and random_id for every combination of data_size, approximation rank and increment\n",
    "    given in the config and saves all the results to a csv file in the results folder (given in the configuration).\n",
    "    \"\"\"\n",
    "    results_log = DataLog(LogFields)  # Initializing an empty results log.\n",
    "    random_svd_with_run_time: Callable = measure_time(random_svd)\n",
    "    random_id_with_run_time: Callable = measure_time(random_id)\n",
    "\n",
    "    for data_size in data_sizes:\n",
    "        data_matrix: Matrix = used_data_factory(data_size, singular_values)\n",
    "\n",
    "        for approximation_rank in approximation_ranks:\n",
    "            next_singular_value: Scalar = singular_values[approximation_rank + 1] if \\\n",
    "                approximation_rank < len(singular_values) else singular_values[-1]\n",
    "\n",
    "            for increment in increments:\n",
    "                # Executing all the tested methods.\n",
    "                U, sigma, VT, svd_duration = random_svd_with_run_time(data_matrix, approximation_rank, increment)\n",
    "                random_svd_accuracy: Scalar = np.linalg.norm(data_matrix - U.dot(np.diag(sigma).dot(VT)))\n",
    "                B, P, id_duration = random_id_with_run_time(data_matrix, approximation_rank, increment)\n",
    "                random_id_accuracy: Scalar = np.linalg.norm(data_matrix - B.dot(P))\n",
    "\n",
    "                # Appending all the experiment results to the log.\n",
    "                results_log.append(LogFields.DataSize, data_size)\n",
    "                results_log.append(LogFields.ApproximationRank, approximation_rank)\n",
    "                results_log.append(LogFields.Increment, increment + approximation_rank)\n",
    "                results_log.append(LogFields.NextSingularValue, next_singular_value)\n",
    "                results_log.append(LogFields.RandomSVDAccuracy, random_svd_accuracy)\n",
    "                results_log.append(LogFields.RandomIDAccuracy, random_id_accuracy)\n",
    "                results_log.append(LogFields.RandomSVDDuration, svd_duration)\n",
    "                results_log.append(LogFields.RandomIDDuration, id_duration)\n",
    "\n",
    "    results_log.save_log(experiment_type + \" results\", results_folder_path=results_path)\n",
    "    print(f'{experiment_type} was performed and results were saved!')\n",
    "    return results_log\n",
    "\n",
    "pd.DataFrame(ex.run().result._data)\n"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "WARNING - Initializing project - No observers have been added to this run\n",
      "INFO - Initializing project - Running command 'main'\n",
      "INFO - Initializing project - Started\n",
      "INFO - Initializing project - Result: <__main__.DataLog object at 0x7ff85c8735c0>\n",
      "INFO - Initializing project - Completed after 2:24:45\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Example No. 1 was performed and results were saved!\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>Data size</th>\n",
       "      <th>increment</th>\n",
       "      <th>K+1 singular value</th>\n",
       "      <th>Random ID Accuracy</th>\n",
       "      <th>Random ID Duration in seconds</th>\n",
       "      <th>Random SVD Accuracy</th>\n",
       "      <th>Random SVD Duration in seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>2.000000e-16</td>\n",
       "      <td>2.864991e-15</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>1.848750e-15</td>\n",
       "      <td>0.001168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>2.000000e-16</td>\n",
       "      <td>2.943775e-15</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>3.510649e-15</td>\n",
       "      <td>0.006946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>10000</td>\n",
       "      <td>10</td>\n",
       "      <td>2.000000e-16</td>\n",
       "      <td>2.917524e-15</td>\n",
       "      <td>0.296181</td>\n",
       "      <td>2.330714e-15</td>\n",
       "      <td>0.462816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    k  Data size  ...  Random SVD Accuracy  Random SVD Duration in seconds\n",
       "0  10        100  ...         1.848750e-15                        0.001168\n",
       "1  10       1000  ...         3.510649e-15                        0.006946\n",
       "2  10      10000  ...         2.330714e-15                        0.462816\n",
       "\n",
       "[3 rows x 8 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 72
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEgjF-B99IVD",
    "colab_type": "text"
   },
   "source": [
    "#Unit-Tests"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LpANy45sVtOS",
    "colab_type": "code",
    "outputId": "89357a74-261e-4a0b-e3b4-17a45e64e026",
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "#@title Test data creation methods\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "test_data_creation.py - tests for data creation methods\n",
    "=======================================================\n",
    "\n",
    "This module contains the tests for the data creation in all the examples.\n",
    "\n",
    "\"\"\"\n",
    "import unittest\n",
    "from scipy.linalg import svdvals\n",
    "\n",
    "\n",
    "class TestDataCreation(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    A class which contains tests for the validity of the created data in all the examples\n",
    "    \"\"\"\n",
    "    def test_example_no_1_data(self):\n",
    "        \"\"\"\n",
    "        Test data creation for Example no. 1\n",
    "\n",
    "        This test validates the data created is ``data_size x data_size``, has rank 20\n",
    "        and posses the expected singular values.\n",
    "\n",
    "        \"\"\"\n",
    "        experiment_type: str = ExperimentType.ExampleNo1\n",
    "        data_size: int = 70\n",
    "        singular_values: RowVector = choose_singular_values(experiment_type)\n",
    "        rank: int = len(singular_values)\n",
    "        data: Matrix = get_data(experiment_type)(data_size, singular_values)\n",
    "        calculated_singular_values: RowVector = svdvals(data, check_finite=False)[:rank]\n",
    "        self.assertTrue(np.allclose(data.shape, (data_size, data_size)))  # Validate data shape.\n",
    "        self.assertEqual(np.linalg.matrix_rank(data, tol=1.8e-16), rank)  # Validate data rank.\n",
    "        self.assertTrue(np.allclose(singular_values, calculated_singular_values))  # Validate singular values.\n",
    "\n",
    "    def test_example_no_2_data(self):\n",
    "        \"\"\"\n",
    "        Test data creation for Example no. 2\n",
    "\n",
    "        This test validates the data created is ``data_size x data_size``, has rank 20\n",
    "        and posses the expected singular values.\n",
    "\n",
    "        \"\"\"\n",
    "        experiment_type: str = ExperimentType.ExampleNo2\n",
    "        data_size: int = 70\n",
    "        singular_values: RowVector = choose_singular_values(experiment_type)\n",
    "        rank: int = len(singular_values)\n",
    "        data: Matrix = get_data(experiment_type)(data_size, singular_values)\n",
    "        calculated_singular_values: RowVector = svdvals(data, check_finite=False)[:rank]\n",
    "        self.assertTrue(np.allclose(data.shape, (data_size, data_size)))  # Validate data shape.\n",
    "        self.assertEqual(np.linalg.matrix_rank(data, tol=singular_values[rank - 1] / 2), rank)  # Validate data rank.\n",
    "        self.assertTrue(np.allclose(singular_values, calculated_singular_values))  # Validate singular values.\n",
    "\n",
    "    def test_example_no_3_data(self):\n",
    "        \"\"\"\n",
    "        Test data creation for Example no. 3\n",
    "\n",
    "        This test validates the data created is ``data_size x data_size``, has rank 60\n",
    "        and posses the expected singular values.\n",
    "\n",
    "        \"\"\"\n",
    "        experiment_type: str = ExperimentType.ExampleNo3\n",
    "        data_size: int = 70\n",
    "        singular_values: RowVector = choose_singular_values(experiment_type)\n",
    "        rank: int = len(singular_values)\n",
    "        data: Matrix = get_data(experiment_type)(data_size, singular_values)\n",
    "        calculated_singular_values: RowVector = svdvals(data, check_finite=False)[:rank]\n",
    "        self.assertTrue(np.allclose(data.shape, (data_size, data_size)))  # Validate data shape.\n",
    "        self.assertEqual(np.linalg.matrix_rank(data, tol=singular_values[rank - 1] / 2), rank)  # Validate data rank.\n",
    "        self.assertTrue(np.allclose(singular_values, calculated_singular_values))  # Validate singular values.\n",
    "\n",
    "    def test_example_no_4_data(self):\n",
    "        \"\"\"\n",
    "        Test data creation for Example no. 4\n",
    "\n",
    "        This test validates the data created is ``data_size x data_size`` and posses the expected singular values.\n",
    "\n",
    "        \"\"\"\n",
    "        experiment_type: str = ExperimentType.ExampleNo4\n",
    "        data_size: int = 80\n",
    "        singular_values: RowVector = choose_singular_values(experiment_type)\n",
    "        known_singular_values_num: int = len(singular_values)\n",
    "        data: Matrix = get_data(experiment_type)(data_size, singular_values)\n",
    "        calculated_singular_values: RowVector = svdvals(data, check_finite=False)[:known_singular_values_num]\n",
    "        self.assertTrue(np.allclose(data.shape, (data_size, data_size)))  # Validate data shape.\n",
    "        self.assertTrue(np.allclose(singular_values, calculated_singular_values))  # Validate singular values.\n",
    "\n",
    "    def test_example_no_5_data(self):\n",
    "        \"\"\"\n",
    "        Test data creation for Example no. 5\n",
    "\n",
    "        This test validates the data created is ``data_size x data_size`` and posses the expected singular value\n",
    "        :math:`10^{-17}` as the second largest singular value with multiplicity of at least ``data_size - 2``.\n",
    "\n",
    "        \"\"\"\n",
    "        experiment_type: str = ExperimentType.ExampleNo5\n",
    "        data_size: int = 70\n",
    "        singular_values: RowVector = choose_singular_values(experiment_type)\n",
    "        known_singular_values_num: int = data_size - 2\n",
    "        data: Matrix = get_data(experiment_type)(data_size, singular_values)\n",
    "        calculated_singular_values: RowVector = svdvals(data, check_finite=False)[1:known_singular_values_num + 1]\n",
    "        known_singular_values: RowVector = singular_values[1] * np.ones(known_singular_values_num)\n",
    "        self.assertTrue(np.allclose(data.shape, (data_size, data_size)))  # Validate data shape.\n",
    "        self.assertTrue(np.allclose(known_singular_values, calculated_singular_values))  # Validate singular values.\n",
    "\n",
    "print(\"Loaded Unit-Tests for data creation successfully!\")\n"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Loaded Unit-Tests for data creation successfully!\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J68jgohxkjKT",
    "colab_type": "code",
    "cellView": "form",
    "outputId": "6706dcb0-04e5-423d-b2a5-fc0d609952bd",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "#@title Test Randomized Agorithms\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "test_random_id.py - tests for Randomized Interpolative Decomposition\n",
    "====================================================================\n",
    "\n",
    "This module contains the tests for the implementation of randomized ID algorithm.\n",
    "\n",
    "\"\"\"\n",
    "from numpy.random import randn as _randn\n",
    "\n",
    "pyximport.install(setup_args={\"include_dirs\": np.get_include()}, reload_support=True)\n",
    "\n",
    "\n",
    "class TestRandomID(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    A class which contains tests for the validity of the random_id algorithm implementation.\n",
    "    \"\"\"\n",
    "    def setUp(self):\n",
    "        \"\"\"\n",
    "        This method sets the variables for the following tests.\n",
    "        \"\"\"\n",
    "        self._m = 100\n",
    "        self._n = 30\n",
    "        self._k = 5\n",
    "        self._increment = 20\n",
    "        self._A = _randn(self._m, self._n)\n",
    "        self._B, self._P = random_id(self._A, self._k, self._increment)\n",
    "        self._approximation = self._B.dot(self._P)\n",
    "\n",
    "    def test_matrices_shapes(self):\n",
    "        \"\"\"\n",
    "        This methods tests the shapes of the matrices :math:`B` and :math:`P` in the decomposition.\n",
    "        \"\"\"\n",
    "        self.assertTrue(self._B.shape, (self._m, self._k))\n",
    "        self.assertTrue(self._P.shape, (self._k, self._n))\n",
    "\n",
    "    def test_interpolative_decomposition(self):\n",
    "        \"\"\"\n",
    "        This methods tests if the decomposition satisfies the properties of interpolative-decomposition.\n",
    "        \"\"\"\n",
    "        self.assertTrue(np.all(self._P <= 2))  # Validate entries of P are between -1 and 2.\n",
    "        self.assertTrue(np.all(self._P >= -2))\n",
    "        # Validate P's norm is bound by the theoretical bound\n",
    "        self.assertLessEqual(np.linalg.norm(self._P), np.sqrt(self._k * (self._n - self._k) + 1))\n",
    "        self.assertGreaterEqual(svdvals(self._P)[-1], 1)  # Validate the least singular value of P is at least 1.\n",
    "\n",
    "        for unit_vector in np.eye(self._k):  # Validate P has kxk identity matrix as a sub-matrix.\n",
    "            self.assertIn(unit_vector, self._P.T)\n",
    "\n",
    "        for col in self._B.T:  # Validate every column of B is also a column of A.\n",
    "            self.assertIn(col, self._A.T)\n",
    "\n",
    "    def test_approximation_estimate(self):\n",
    "        \"\"\"\n",
    "        This methods tests if the random ID satisfies the theoretical bound. There is a probability\n",
    "        of less then :math:`10^{-17}` this bound won't be satisfied...\n",
    "        \"\"\"\n",
    "        real_sigmas = np.linalg.svd(self._A, full_matrices=False, compute_uv=False)\n",
    "        estimate_error = np.linalg.norm(self._A - self._approximation)\n",
    "        expected_bound = 10 * np.sqrt(self._n * (self._k + self._increment) * self._m * self._k)\n",
    "        expected_bound *= real_sigmas[self._k]\n",
    "        self.assertLessEqual(estimate_error, expected_bound)\n",
    "\n",
    "print(\"Loaded Unit-Tests for Random ID successfully!\")\n"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Loaded Unit-Tests for Random ID successfully!\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4J6uzcsdHTH0",
    "colab_type": "code",
    "cellView": "form",
    "outputId": "7564ec31-56bd-4425-8c07-766b6f120f50",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "#@title Test Randomized SVD\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "test_random_svd.py - tests for Randomized Singular-Value Decomposition\n",
    "======================================================================\n",
    "\n",
    "This module contains the tests for the implementation of randomized SVD algorithm.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "pyximport.install(setup_args={\"include_dirs\": np.get_include()}, reload_support=True)\n",
    "\n",
    "\n",
    "class TestRandomSVD(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    A class which contains tests for the validity of the random_svd algorithm implementation.\n",
    "    \"\"\"\n",
    "    def setUp(self):\n",
    "        \"\"\"\n",
    "        This method sets the variables for the following tests.\n",
    "        \"\"\"\n",
    "        self._m = 100\n",
    "        self._n = 30\n",
    "        self._k = 5\n",
    "        self._increment = 20\n",
    "        self._A = _randn(self._m, self._n)\n",
    "        self._U, self._sigma, self._VT = random_svd(self._A, self._k, self._increment)\n",
    "        self._approximation = self._U.dot(np.diag(self._sigma).dot(self._VT))\n",
    "\n",
    "    def test_matrices_shapes(self):\n",
    "        \"\"\"\n",
    "        This methods tests the shapes of the matrices :math:`U` and :math:`V` in the decomposition.\n",
    "        \"\"\"\n",
    "        self.assertTrue(self._U.shape, (self._m, self._k))\n",
    "        self.assertTrue(self._VT.shape, (self._k, self._n))\n",
    "\n",
    "    def test_matrices_svd_decomposition(self):\n",
    "        \"\"\"\n",
    "        This methods tests if the output decomposition satisfies the properties of SVD decomposition.\n",
    "        \"\"\"\n",
    "        self.assertTrue(np.allclose(self._U.T.dot(self._U), np.eye(self._k)))\n",
    "        self.assertTrue(np.allclose(self._VT.dot(self._VT.T), np.eye(self._k)))\n",
    "        self.assertTrue(np.all(self._sigma > 0))\n",
    "\n",
    "    def test_decomposition_rank(self):\n",
    "        \"\"\"\n",
    "        This methods tests if the number of positive singular values is equal to the approximation rank.\n",
    "        \"\"\"\n",
    "        self.assertEqual(len(self._sigma), self._k)\n",
    "\n",
    "    def test_approximation_estimate(self):\n",
    "        \"\"\"\n",
    "        This methods tests if the random SVD satisfies the theoretical bound. There is a probability\n",
    "        of less then :math:`10^{-17}` this bound won't be satisfied...\n",
    "        \"\"\"\n",
    "        real_sigmas = np.linalg.svd(self._A, full_matrices=False, compute_uv=False)\n",
    "        estimate_error = np.linalg.norm(self._A - self._approximation)\n",
    "        expected_bound = 10 * np.sqrt(self._n * (self._k + self._increment))\n",
    "        expected_bound *= real_sigmas[self._k]\n",
    "        self.assertLessEqual(estimate_error, expected_bound)\n",
    "\n",
    "print(\"Loaded Unit-Tests for Random SVD successfully!\")\n"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Loaded Unit-Tests for Random SVD successfully!\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ad8IdeM9HqCa",
    "colab_type": "code",
    "cellView": "form",
    "outputId": "9d410231-45af-4b0b-c55f-6939114795f2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    }
   },
   "source": [
    "#@title Running all these Unit-Tests\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)\n"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "test_example_no_1_data (__main__.TestDataCreation) ... ok\n",
      "test_example_no_2_data (__main__.TestDataCreation) ... ok\n",
      "test_example_no_3_data (__main__.TestDataCreation) ... ok\n",
      "test_example_no_4_data (__main__.TestDataCreation) ... ok\n",
      "test_example_no_5_data (__main__.TestDataCreation) ... ok\n",
      "test_approximation_estimate (__main__.TestRandomID) ... ok\n",
      "test_interpolative_decomposition (__main__.TestRandomID) ... ok\n",
      "test_matrices_shapes (__main__.TestRandomID) ... ok\n",
      "test_approximation_estimate (__main__.TestRandomSVD) ... ok\n",
      "test_decomposition_rank (__main__.TestRandomSVD) ... ok\n",
      "test_matrices_shapes (__main__.TestRandomSVD) ... ok\n",
      "test_matrices_svd_decomposition (__main__.TestRandomSVD) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 0.083s\n",
      "\n",
      "OK\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7ff85c3da550>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 50
    }
   ]
  }
 ]
}